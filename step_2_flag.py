# -*- coding: utf-8 -*-
"""Step_2_Flag.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1APrmxh6y4zO1oCrWTSCvPVWYOSPTRiA8
"""

!pip install pandas
!pip install tweepy
!pip install vaderSentiment #VaderSentiment
!pip install xlsxwriter

import pandas as pd
from nltk.sentiment.vader import SentimentIntensityAnalyzer
import nltk
nltk.download('vader_lexicon')

tesla_tweets = pd.read_excel (r'/content/tesla.xlsx')


listy = [] # Create a empty list to store all of the tweets and polarity
tweet_list = [] # Copying tweets


sid = SentimentIntensityAnalyzer() # Putting sid into a function

for index, row in tesla_tweets.iterrows():
  row['Tweets'] = row['Tweets'].replace("Tesla", "Tomato") #string.replace("geeks", "Geeks"))
  row['Tweets'] = row['Tweets'].replace("Model", "Phanith") #string.replace("geeks", "Geeks"))
  tweet_list.append(row['Tweets'])
  ss = sid.polarity_scores(row['Tweets'])
  listy.append(ss) # Loop

se = pd.Series(listy)
tweet_list_series = pd.Series(tweet_list)

tesla_tweets['Tweets'] = tweet_list_series.values
tesla_tweets['polarity'] = se.values

datatoexcel = pd.ExcelWriter("/content/tesla_Polarity.xlsx", engine = 'xlsxwriter')

tesla_tweets.to_excel(datatoexcel, sheet_name = 'Sheet1')

datatoexcel.save()

words = pd.read_excel (r'/content/words.xlsx')
flagbag = [] # Create a list of words that we want to capture


for index, row in tesla_tweets.iterrows():
  for index2, row2 in words.iterrows():
    if (row2['Word'] in row['Tweets']):
        flagbag.append(row['Tweets'])

df = pd.DataFrame(flagbag)
writer = pd.ExcelWriter('/content/flag_words.xlsx', engine='xlsxwriter')
df.to_excel(writer, sheet_name='welcome', index=False)
writer.save()